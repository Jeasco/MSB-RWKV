# MSB-RWKV
## Coming Soon!
Exploring long-range dependencies is crucial for infrared image super-resolution. Although Transformers can model long-range interactions, yet their application is limited by quadratic complexity, especially for high-resolution infrared images. Recently, the RWKV model, which has gained prominence in the NLP domain due to its efficiency in handling long sequences, offers a promising solution. To harness the benefits of this model for infrared image super-resolution, we introduce a multiple span bidirectional RWKV network, termed MSB-RWKV, to to efficiently model long-range dependencies and local context restoration clues in infrared images. Given that RWKV was originally optimized for 1D sequence processing, we implemented two critical modifications to adapt it for 2D infrared images. First, we propose the multiple spin bidirectional WKV (MSB-WKV) attention mechanism. This mechanism captures global dependencies with linear computational complexity by incorporating both bidirectional and multiple span attention. The bidirectional attention facilitates a comprehensive global receptive field, while multiple span attention effectively models 2D spatial dependencies from various scanning directions. Second, we introduce a wide token shift (Wide Shift) layer, which enhances local dependencies by shifting tokens in multiple directions and across an extensive context range. This layer improves the model's ability to capture fine-grained details and contextual information in Infrared images. These innovations collectively position MSB-RWKV as a highly efficient and effective model for infrared image super-resolution. Comprehensive experiments validate that MSB-RWKV outperforms state-of-the-art methods, demonstrating its superior performance.
